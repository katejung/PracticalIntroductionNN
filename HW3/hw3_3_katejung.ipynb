{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "In the <b>HW3_template</b> folder you will find `TSLA.csv`, `GOOGL.csv` and `DJI.csv` files. Use Pandas (You have used it in HW1) to retrieve the dataset. Use only <b>Open</b> price as your input. (You will train three models for three different stocks, don't mix these data together!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date       Open   High        Low      Close  Adj Close    Volume\n",
      "0  2010-06-29  19.000000  25.00  17.540001  23.889999  23.889999  18766300\n",
      "1  2010-06-30  25.790001  30.42  23.299999  23.830000  23.830000  17187100\n",
      "2  2010-07-01  25.000000  25.92  20.270000  21.959999  21.959999   8218800\n",
      "3  2010-07-02  23.000000  23.10  18.709999  19.200001  19.200001   5139800\n",
      "4  2010-07-06  20.000000  20.00  15.830000  16.110001  16.110001   6866900\n"
     ]
    }
   ],
   "source": [
    "data_TSLA = pd.read_csv(\"TSLA.csv\") \n",
    "data_GOOGL = pd.read_csv(\"GOOGL.csv\")\n",
    "daat_DJI = pd.read_csv(\"DJI.csv\")\n",
    "data_TSLA = data_TSLA.sort_values('Date')\n",
    "\n",
    "print(data_TSLA.head())\n",
    "[n, _] = data_TSLA.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize your data\n",
    "You could use `MinMaxScaler` in `sklearn.preprocessing` to normalize the data between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessData(data, num_seq=20):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    open_scaled =  scaler.fit_transform(data_TSLA['Open'].values.reshape(-1,1))\n",
    "#     data_processed = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Open'])\n",
    "#     data_processed['Date'] = data['Date']\n",
    "#     data_processed['Open'] = data['Open']\n",
    " \n",
    "    n =len(data)\n",
    "    x, y = [], []\n",
    "    \n",
    "    for i in range(num_seq, n):\n",
    "        x.append(open_scaled[i-num_seq:i, 0])\n",
    "        y.append(open_scaled[i, 0])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    x = np.reshape(x, (-1, num_seq, 1))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training, validation and testing data\n",
    "<p style=\"font-size:20px\">Since you will impelement a many-to-one Recurrent Neural Network model, every input data will have shape [batch_size, num_seq, input_size] and output data will have shape [batch_size, input_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_seq = 20;\n",
    "[x, y] = preprocessData(data_TSLA)\n",
    "N = len(x)\n",
    "test_split = int(N*.90)\n",
    "val_split = int(N*.75)\n",
    "train_x, train_y = x[:val_split], y[:val_split]\n",
    "val_x, val_y = x[val_split:test_split], y[val_split:test_split]\n",
    "test_x, test_y = x[test_split:], y[test_split:]\n",
    "train_dates, val_dates, test_dates = data_TSLA[:val_split], data_TSLA[val_split:test_split], data_TSLA[test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TesnorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size=1\n",
    "lstm_size=128\n",
    "num_layers=1\n",
    "keep_prob=0.8\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "init_epoch = 5\n",
    "num_epoch = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "lstm_graph = tf.Graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_seq, 1], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.unstack(X, num_seq, axis = 1)\n",
    "\n",
    "with tf.name_scope('rnn'):\n",
    "    # Define a lstm cell\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0)\n",
    "    # Get LSTM cell outputs and states\n",
    "    rnn_outputs, rnn_states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    # For tensorboard\n",
    "    \n",
    "with tf.name_scope('nn'):\n",
    "    # Define the first ANN\n",
    "    nn_outputs = tf.layers.dense(rnn_outputs[-1], 64 , activation=tf.nn.tanh, name = 'nn1')\n",
    "    # Define the second ANN\n",
    "    prediction = tf.layers.dense(nn_outputs, 1, name = 'nn2')\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(prediction - Y), name = 'loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name = 'optimizer').minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained 100 vadliation accuracy0.0199089\n",
      "trained 200 vadliation accuracy0.0502842\n",
      "trained 300 vadliation accuracy0.0775987\n",
      "trained 400 vadliation accuracy0.0530007\n",
      "trained 500 vadliation accuracy0.0580432\n",
      "trained 600 vadliation accuracy0.06533\n",
      "trained 700 vadliation accuracy0.0631326\n",
      "trained 800 vadliation accuracy0.0670445\n",
      "trained 900 vadliation accuracy0.0650421\n",
      "trained 1000 vadliation accuracy0.0644573\n",
      "Final Validation Accuracy: 0.0644573\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epoch):\n",
    "        for start in range(0, len(train_x) + 1, batch_size):\n",
    "            # Create training batches\n",
    "            batch_x = train_x[start:min(start+batch_size, len(train_x))]\n",
    "            batch_y = train_y[start:min(start+batch_size, len(train_y))]\n",
    "\n",
    "            # Run optimization operation (backprop)\n",
    "            sess.run(optimizer, feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "        if (i+1)%100 == 0:\n",
    "            acc = sess.run(loss,feed_dict={X:val_x, Y:val_y})\n",
    "            print(\"trained \"+str(i+1)+\" vadliation accuracy\" + str(acc))\n",
    "    print(\"Final Validation Accuracy:\", sess.run(loss, feed_dict={X:val_x, Y:val_y}))\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
